{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# import scipy.io as sio\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dir = '../data/train/'\n",
    "    test_dir = '../data/test/'\n",
    "    categories = ['happy', 'sad', 'fear', 'surprise', 'neutral', 'angry', 'disgust']\n",
    "\n",
    "    train_file_dictionary = {}\n",
    "    train_imagefile_to_class_dictionary = {}\n",
    "    for emotion in categories:\n",
    "        train_file_dictionary[emotion] = []\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(categories)):\n",
    "        for subdir, dirs, files in os.walk(train_dir+categories[i]+'/'):\n",
    "            for file in files:\n",
    "                train_file_dictionary[categories[i]].append(train_dir+categories[i]+'/'+file)\n",
    "                train_imagefile_to_class_dictionary[counter] = {}\n",
    "                train_imagefile_to_class_dictionary[counter]['file'] = train_dir+categories[i]+'/'+file\n",
    "                train_imagefile_to_class_dictionary[counter]['label'] = i\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "\n",
    "    test_file_dictionary = {}\n",
    "    test_imagefile_to_class_dictionary = {}\n",
    "    for emotion in categories:\n",
    "        test_file_dictionary[emotion] = []\n",
    "\n",
    "    counter = 0\n",
    "    for i in range(len(categories)):\n",
    "        for subdir, dirs, files in os.walk(test_dir+categories[i]+'/'):\n",
    "            for file in files:\n",
    "                test_file_dictionary[categories[i]].append(test_dir+categories[i]+'/'+file)\n",
    "                test_imagefile_to_class_dictionary[counter] = {}\n",
    "                test_imagefile_to_class_dictionary[counter]['file'] = test_dir+categories[i]+'/'+file\n",
    "                test_imagefile_to_class_dictionary[counter]['label'] = i\n",
    "                counter += 1\n",
    "\n",
    "    return train_imagefile_to_class_dictionary, test_imagefile_to_class_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialEmotionDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, imagefile_to_class_dictionary, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "            imagefile_to_class_dictionary (dictionary): Dictionary of image filenames to class for each emotion.\n",
    "        \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "        self.imagefile_to_class_dictionary = imagefile_to_class_dictionary\n",
    "        self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize((96, 96)),\n",
    "                    transforms.ToTensor(),\n",
    "#                     transforms.CenterCrop(10),\n",
    "                 \n",
    "                 transforms.Normalize((0.5), \n",
    "                                      (0.5))])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagefile_to_class_dictionary.keys())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         print(\"idx\", idx)\n",
    "        path_to_image = self.imagefile_to_class_dictionary[idx]['file']\n",
    "#         image = io.imread(path_to_image)\n",
    "        image = Image.open(path_to_image)\n",
    "        image = self.transform(image).float()\n",
    "        label = int(self.imagefile_to_class_dictionary[idx]['label'])\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceNet, self).__init__()\n",
    "        # torch.Size([256, 1, 64, 64])\n",
    "        # 3 input image channel (RGB), #6 output channels, 4x4 kernel \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(3,3), stride=1, \n",
    "                               padding=1, dilation=1, groups=1, \n",
    "                               bias=True, padding_mode='reflect')\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 96, kernel_size=(3,3), stride=1, \n",
    "                               padding=1, dilation=1, groups=1, \n",
    "                               bias=True, padding_mode='reflect')\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(96, 256, kernel_size=(3,3), stride=1, \n",
    "                               padding=1, dilation=1, groups=1, \n",
    "                               bias=True, padding_mode='reflect')\n",
    "        \n",
    "        \n",
    "        self.drop1 = nn.Dropout(p=0.1)\n",
    "        self.norm1 = nn.LayerNorm([48, 48])\n",
    "        self.norm2 = nn.LayerNorm([24, 24])\n",
    "        \n",
    "        self.fc1 = nn.Linear(36864, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1028)\n",
    "        self.fc3 = nn.Linear(1028, 7)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = self.norm1(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = self.norm2(x)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = torch.flatten(x, 1)\n",
    "#         print(x.shape)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "#         x = self.drop1(x)\n",
    "        \n",
    "#         output = x\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imagefile_to_class_dictionary, test_imagefile_to_class_dictionary = load_dataset()\n",
    "train_dataset = FacialEmotionDataset(train_imagefile_to_class_dictionary)\n",
    "test_dataset = FacialEmotionDataset(test_imagefile_to_class_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_FaceNet():\n",
    "    train_imagefile_to_class_dictionary, test_imagefile_to_class_dictionary = load_dataset()\n",
    "    train_dataset = FacialEmotionDataset(train_imagefile_to_class_dictionary)\n",
    "    test_dataset = FacialEmotionDataset(test_imagefile_to_class_dictionary)\n",
    "\n",
    "\n",
    "    print(len(train_dataset))\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              shuffle=True,\n",
    "                                             )\n",
    "    \n",
    "    # Parameters\n",
    "    max_epochs = 10\n",
    "    lr = 0.01\n",
    "    momentum = 0.9\n",
    "\n",
    "    # CUDA for PyTorch\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    print(\"cuda device = \", device)\n",
    "\n",
    "\n",
    "    face_net = FaceNet().double()\n",
    "    # Try different optimzers here [Adam, SGD, RMSprop]\n",
    "    optimizer = optim.SGD(face_net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "    training_losses = []\n",
    "\n",
    "    # Generators\n",
    "    training_set = train_dataset\n",
    "    training_generator = train_data_loader\n",
    "\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "    # loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    face_net.train()\n",
    "\n",
    "    # Loop over epochs\n",
    "    print(\"Beginning Training..................\")\n",
    "    for epoch in range(max_epochs):\n",
    "        # print(\"epoch: \", epoch)\n",
    "        # Training\n",
    "        total_epoch_loss = 0\n",
    "        for batch_idx, (batch_data, batch_labels) in enumerate(train_data_loader):\n",
    "            print('batch_idx = ', batch_idx)\n",
    "            batch_data = batch_data.double()\n",
    "            batch_labels = batch_labels\n",
    "\n",
    "            predicted_output = face_net(batch_data)\n",
    "\n",
    "            predicted_output = predicted_output.double()                                \n",
    "            target_output = batch_labels\n",
    "\n",
    "            # print(predicted_output)\n",
    "            # print()\n",
    "            # print(target_output)\n",
    "\n",
    "\n",
    "\n",
    "            loss = loss_fn(predicted_output, target_output)\n",
    "    #         loss = F.nll_loss(predicted_output, target_output)   # Compute loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()  \n",
    "\n",
    "            total_epoch_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 25 == 0:\n",
    "                print('Train Epoch: {} \\tLoss: {:.6f}'.format(\n",
    "                    epoch, total_epoch_loss))\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            with open('../saved_models/face_network_1.pkl', 'wb') as f:\n",
    "                torch.save(face_net.state_dict(), f)\n",
    "\n",
    "        training_losses.append(total_epoch_loss)\n",
    "\n",
    "    with open('../saved_models/face_network_1_final.pkl', 'wb') as f:\n",
    "        torch.save(face_net.state_dict(), f)\n",
    "\n",
    "    with open('../saved_models/face_network_1_losses.npy', 'wb') as f:\n",
    "        np.save(f, np.array(training_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28709\n",
      "cuda device =  cpu\n",
      "Beginning Training..................\n",
      "batch_idx =  0\n",
      "Train Epoch: 0 \tLoss: 1.881925\n",
      "batch_idx =  1\n",
      "batch_idx =  2\n",
      "batch_idx =  3\n",
      "batch_idx =  4\n",
      "batch_idx =  5\n",
      "batch_idx =  6\n",
      "batch_idx =  7\n",
      "batch_idx =  8\n",
      "batch_idx =  9\n",
      "batch_idx =  10\n",
      "batch_idx =  11\n",
      "batch_idx =  12\n",
      "batch_idx =  13\n",
      "batch_idx =  14\n",
      "batch_idx =  15\n",
      "batch_idx =  16\n"
     ]
    }
   ],
   "source": [
    "train_FaceNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
